{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Neural Net basics: Part One"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "If you're like me, learning about neural net could be pretty confusing. As a curious person who just wants to see this so called _neural network_ in action. Materials on the internet about neural networks could be so confusing (too mathematical or too much code). So I decided to make the best of both. Teach you what you'll need to get started developing neural networks.\n",
    "\n",
    "Let's build a two layer neural network to predict the mappings between binary digits. The only dependency needed is [`numpy`](https://pypi.python.org/pypi/numpy). If you're not familiar with [`numpy`](http://www.numpy.org/) it's okay. You'll still be able to follow up just fine!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "After installing numpy by running `pip install numpy` or downloading it [here](https://pypi.python.org/pypi/numpy). We're gonna want to import `numpy`. It's common convention to import numpy as `np`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# The only dependency required.\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let us define our two inputs, shall we."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X = np.array([ [0, 0, 1], [0, 1, 1], [1, 0, 1], [1, 1, 1] ])\n",
    "y = np.array([ [0], [1], [1], [0] ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "`X` is our input/feature, while `y` is our target/label. Features in machine learning are the relevant information about a piece of data. for example: if I want to predict the score of the next soccer match, it'll be nice if I have player's statistics and past match history (I'm not a soccer fan). Those statistics represent our features while the score can be our target/labels, since that's what we want to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now that we understand what features and labels are. It's time to define some [hyperparameters](https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)). Hyperparameters in machine learning are the tuning knobs for our network. For instance, how many layers do we want? How many training iterations do we want to perform? Usually, one wouldn't know what the best hyperparameters are right off the bat. You'll need some tweaking to know the best set of hyperparameters to use. In this case we're gonna keep it simple. Let's define our _input dimension_, _hidden dimension_ and _output dimension_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dimension: 3\n",
      "Hidden dimension: 4\n",
      "Ouput dimension: 1\n"
     ]
    }
   ],
   "source": [
    "input_dim = X.shape[-1]\n",
    "hidden_dim = 4\n",
    "output_dim = y.shape[-1]\n",
    "print('Input dimension: {}\\nHidden dimension: {}\\nOuput dimension: {}'.format(input_dim, hidden_dim, output_dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "W0 = 2 * np.random.random(size=[input_dim, hidden_dim]) - 1\n",
    "W1 = 2 * np.random.random(size=[hidden_dim, output_dim]) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n",
    "We will choose the [sigmoid activation function ](https://en.wikipedia.org/wiki/Sigmoid_function) as our _\"nonlinearity\"_. While it can be several kinds of functions. A sigmoid function maps any value to a value between 0 and 1. We use it to convert numbers to probabilities. It also has several other desirable properties for training neural networks.![Sigmoid](images/sigmoid.png)\n",
    "\n",
    "In order to \"backpropagate\" (which we'll talk about in a second) we are going to find the derivative of the sigmoid function. This can be represented in Python as just one line of code. #smiles\n",
    "\n",
    "![Sigmoid derivative](images/sigmoid-deriv-2.png)\n",
    "If you're unfamiliar with derivatives, just think about it as the slope of the sigmoid function at a given point (as you can see above, different points have different slopes). For more on derivatives, check out this [derivatives tutorial from Khan Academy](https://www.khanacademy.org/math/differential-calculus/taking-derivatives/derivative_intro/v/calculus-derivatives-1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(Z, prime=False):\n",
    "    if prime:\n",
    "        return Z * (1 - Z)\n",
    "    return 1 / (1 + np.exp(-Z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's training time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for i in range(10000):\n",
    "    # Forward propagte our input to get a prediction\n",
    "    l1 = sigmoid(np.dot(X, W0))\n",
    "    l2 = sigmoid(np.dot(l1, W1))\n",
    "    \n",
    "    # Estimate how much we missed\n",
    "    l2_error = (y - l2) ** 2\n",
    "    l2_delta = l2_error * sigmoid(l2, prime=True)\n",
    "    \n",
    "    # Back propagate error in layer 2 into layer 1\n",
    "    l1_error = np.dot(l2_delta, W1.T)\n",
    "    l1_delta = l1_error * sigmoid(l1, prime=True)\n",
    "    \n",
    "    # Update weights\n",
    "    W1 += np.dot(l1.T, l2_delta)\n",
    "    W0 += np.dot(X.T, l1_delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to start training this Neural net! Here's the plan: We are going to multiply our input with the first set of weights _(line 3)_. We'll then pass the result to the next \"layer\" to multiply with the next weight _(line 4)_. At this point we have our prediction. But our prediction would (usually) be wrong. So we're going to estimate by how much we missed i.e. calculate error between our prediction and the actual output _(line 7)_.\n",
    "\n",
    "![Our neural network](images/neural-net.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to know how bad our network performed in layer two we'll need to do an _element wise multiplication_ of our layer 2's error with our sigmoid derivative (Good thing we already wrote the function line 8). Now we know how bad our network performed in layer two.\n",
    "\n",
    "Since layer 2 depends on layer 1, we would need to know how bad layer 1 is also. Because if layer 1 get it wrong, layer 2 will also do poorly. Therefore we'll pass the error we got in layer 2 to layer 1 by matrix multiplying layer 2's error with the weight transposed that connects both layers _(line 11)_. After this we can do an _element wise multiplication_ of the error with the sigmoid's derivative _(line 12)_.\n",
    "\n",
    "Now that we have our error propagated to the first layer, it's time to update our weights, such that in the next iteration we won't make the mistake we made at the current time step.\n",
    "This whole process is called \"Backpropagation\". The backpropagation algorithm is the reason why our neural network works so well. And sadly, it's the reason why neural networks generally take days or weeks to train as your dataset scales.\n",
    "\n",
    "This whole process is called [**“Backpropagation”**](https://brilliant.org/wiki/backpropagation/). The backpropagation algorithm is the reason why our neural network works so well. And sadly, it’s the reason why neural networks generally take days or weeks to train as your dataset scales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "We looked at how to build a very simple neural network without too much Math or too much code (just as promised). This is the first step to building Software 2.0 (like Andrej Karpathy said). Try to modify the dataset of switch datasets and see what happens. You can also try printing out the weights and the layer’s results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
